---
title: "exercises_week9"
author: "Lindsey Greenhill"
date: "3/24/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(stargazer)
library(tidyverse)

# reading in the data

df <-foreign::read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/risky.behavior/risky_behaviors.dta")

# making variable into integer

df <- df %>%
  mutate(fupacts = as.integer(fupacts))
```


# Question 1

## Part a

```{r}

# have to include both women_alone and couples as treatments for the model

mod_a  <- glm(fupacts ~ women_alone + couples, data = df, family = "poisson")

# summary of the model

summary(mod_a)

# look at the deviance residuals. The min and the max should be relatively equal
# in absolute value. If they aren't, it hints at overdispersion.

# look at the deviances:  roughly 240 on 1 degree of freedom. We are way in the
# tail. So much so that we don't have to do the p chisq thing.

#ssr <- (df$fupacts - mod_a$fitted.values)^2/sqrt(mod_a$fitted.values)
#sum_ssr <- sum(ssr^2)

# k is the number of variables that you add over the mean model
# 1 df. Value is way into the tail, so there is a problem. 



#over <- 1/(nrow(df) - mod_a$df.null -  mod_a$df.residual)*sum_ssr

# I'm going with the code from the book. It's more correct and makes more sense

yhat <- predict(mod_a, type = "response")
z <- (df$fupacts - yhat) / sqrt(yhat)

# cat("overdispersion ratio is", sum(z^2)/(nrow(df) - 3), "\n")

# cat("p-value of overdispersion test is", pchisq(sum(z^2), nrow(df) - 3), "\n")

# there is evidence of overdispersion


```
### Model fit

* The treatment variables appears to be statistically significant

* There is a significant difference between the null deviance and the residual
deviance, indicating that the model is better than the null model

### Overdisperion check

There are a few ways to check for overdispersion. A quick way to check is to
look at the minimum and maximum deviance residuans. If the model is a good fit,
the min and max should be relatively equal in absolute value.

* The minimum deviance residual = -6.6 while the maximum deviance residual = 27.2.
These two values differ substantially in absolute value and hints at
overdispersion

Another way to check for overdispersion is to perform a chi squared test on the
overdispersion ratio. If the overdisperion ratio is in the tail of the
distribution, then there is likely overdispersion

* The overdispersion ratio is `r sum(z^2)/(nrow(df) - 3)`

* The p value of the overdispersion test is `r pchisq(sum(z^2), nrow(df) - 3)`

* The p value is in the tail of the chi squared distribution, suggesting that
there is overdispersion



## Part b

In the code below I extend the model to include pre-treatment variables.


```{r}
# creating model with pre treatment variables. It will still probably be
# overdispersed

mod_b <- glm(fupacts ~ women_alone  + sex + couples + bs_hiv + bupacts, 
             data = df,
             family = "poisson")

summary(mod_b)

# look at the deviance residuals. The min and the max should be relatively equal
# in absolute value. If they aren't, it hints at overdispersion.

# doing the same overdispersion test as in part a

yhat_b <- predict(mod_b, type = "response")

z_b <- (df$fupacts - yhat_b) / sqrt(yhat_b)


```

### Model fit

* All of the variables appear to be statistically significant

* There is a significant difference between the null deviance and the residual
deviance, indicating that the model is improved from the model in part a

### Overdisperion check

* The minimum and maximum deviance residuals are closer in absolute value than
they were in mod_a, however, they differ by 5, hinting at overdispersion

* The overdispersion ratio is `r sum(z_b^2)/(nrow(df) - 6)`

* The p value of the overdispersion test is `r pchisq(sum(z_b^2), nrow(df) - 6)`

* The p value is in the tail of the chi squared distribution, suggesting that
there is overdispersion


## Part c

In the code below I fit an overdispersed Poission model using the glm.nm() function. 

```{r}

#  fitting a negative binomial

mod_c  <- glm.nb(fupacts ~ women_alone  + sex + couples + bs_hiv + bupacts, data = df)

summary(mod_c)

# really changes the  deviance residuals range. the min and max are pretty much
# equal now (abs value). Sex and couples become not statistically significant.


```
### Effectiveness of intervention

We assume that the glm.nm function fixes the overdispersion problem from the
poisson models above

* The coefficient estimates change from mod_a/mod_b to modc, suggesting that the
new model may have been an effective intervention

* Two variables that were statistically significant in the previous models are
now not statistically significant (sexman and couples), suggesting that this
model may have more accurately calculated the standard errors and was thus an
effective intervention


### Interpretation of some coefficients

We can interprety the coefficients in the model using the formula 1/(exp(beta)),
where an increase in 1 in the variables leads to a 1/(exp(beta)) change in the
outcome variable

* Women_alone: Using the above formula, an increase in the women_along variables
(or switching from non treatment to treatment) results in a 1/(exp(-.72)), or `r 1/(exp(-.72))`
increase in unprotected sex acts on average, holding all else
constant

* bs_hivpositive: those who have a positive baseline hiv status on average
engaged in 1/(exp(.02248), or `r 1/(exp(.02248))`, more unprotected sex acts,
holding all else constant





## Part d

I have concerns about the IID Arrivals assumption (the assumption that the
observations are independent and identically distributed). I would expect the
sexual activities of couples are not independent and are highly correlated.
Because the data includes responses from both men and women in the same couple,
the observations are probably not completely independent.


# Question 2

```{r include=FALSE}

# need this library to do the regression

library(nnet)
library(sjPlot)

# reading in data

nes <- foreign::read.dta("http://www.stat.columbia.edu/~gelman/arm/examples/nes/nes5200_processed_voters_realideo.dta") %>% 
  filter(year == 2000) %>%
  select(partyid3,age, gender, race, educ1, income, ideo7)

# recoding and cleaning data. Taken from a pset a while back

nes_clean <- nes %>%
  drop_na() %>%
  mutate(
    gender = as.numeric(str_sub(gender, 1, 1)) - 1,
    educ1 = as.double(str_sub(educ1, 1, 1)),
    partyid3 = as_factor(str_sub(partyid3, 1, 1)),
    income = as.double(str_sub(income, 1, 1)),
    ideo7 = as.double(str_sub(ideo7, 1, 1)),
    race = as_factor(str_sub(race, 1, 1))
  ) %>%
  mutate(partyid3 = if_else(partyid3 == 3, "R",
                            if_else(partyid3 == 2, "I", "D"))) %>%
  mutate(race = case_when(race == 1 ~ "white",
                          race == 2 ~ "black",
                          race == 3 ~ "asian",
                          race == 4 ~ "native_american",
                          race == 5 ~ "hispanic"))

nes_clean_simple <- nes_clean %>%
 select(-race, -gender)
```

## Part a

```{r}

# using the method from class

mod_2 <- multinom(partyid3 ~ age + educ1 + income + ideo7,
                  data = nes_clean_simple)

# using stargazer to display the model

stargazer(mod_2, type = "text")


# using the plot model function to show the coefficients for the model

plot_model(mod_2,
           type = "est",
           title = "Multinomial Model Results")

# interpret the results using the divide by 4 rule!

```

## Part b

The variables have different levels of statistical significance for I and R. I
will interpret the statistically significant coefficients below using the divide
by 4 rule. Although this doesn't give an exact interpretation of the
coefficients, it is much quicker to calculate.

### Coefficient Interpretations: I

The coefficients for age, educ1, and income are not statistically significant
for the Independent coefficients. Ideology is the only statistically significant
variable.

* Using the divide by 4 rule, a 1 point increase on the ideology scale is
associated with roughly an 11% increase on average in the likelihood of being an
Independent compared to a Democrat, holding all else constant.

### Coefficient Interpretations: R

The coefficients for  educ1, income, and ideo7 are statistically significant for
the Republican coefficients. Age is the only non statistically significant
variable.  In summary, the effects of education, income, and ideology have
greater impacts on the likelihood of being a Republican compared to a Democrat
verses the likihood of being an Independent compared to a Democrat.

* Using the divide by 4 rule, a 1 point increase on the ideology scale is
associated with roughly an 19% increase on average in the likelihood of being a
Republican compared to a Democrat, holding all else constant.

* Using the divide by 4 rule, a 1 point increase on the education scale is
associated with roughly an 12% increase on average in the likelihood of being a
Republican compared to a Democrat, holding all else constant.

* Using the divide by 4 rule, a 1 point increase on the income scale is
associated with roughly a 8% increase on average in the likelihood of being a
Republican compared to a Democrat, holding all else constant.

## Part c

For part c, I chose two test cases from the data frame to make predictions off of

```{r}

# this row is someone who is 63, has some college, 34 to 67th percentile income, and is slightly conservative. They are actually a D. 

pred_df <- nes_clean_simple %>%
  slice(1) %>%
  select(-partyid3)

# The model incorrectly predicts this person to be R

predict(mod_2, pred_df)

# case 2. This person is 40, has some college, 34 to 67th percentile income, and is conservative. They are actually a R. 

pred_df_2 <- nes_clean_simple %>%
  slice(2)  %>%
  select(-partyid3)

# The model correctly predicts this person is R

predict(mod_2, pred_df_2)
```

### Test case 1

* This prediction is for someone who is 63, has some college education, is in
the 34 to 67th percentile income, and is slightly conservative.

* The model predicts that thiis person is a Republican

* The model is incorrect -- this person was actually a Democrat

### Test case 2

* This prediction is for someone who is 40, has some college education, is in
the 34 to 67th percentile income, and is conservative.

* The model predicts that this person is a Republican

* The model is correct -- this person was actually a Republican


