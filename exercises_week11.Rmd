---
title: "exercises_week11"
author: "Lindsey Greenhill"
date: "4/14/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(contrasts = rep("contr.treatment",2))
save.image()
library(MASS)
library(stargazer)
library(gt)
library(skimr)
library(mice)
library(arm)
library(tidyverse)
set.seed(123)
```


# Question 25.2

## Part a

```{r}

# using built in iris data set. filtering to one species and selecting petal
# length and width. Petal length is x petal width is y.

iris_2 <- iris %>%
  filter(Species == "setosa") %>%
  select(Petal.Length, Petal.Width)


# MAR data. going to delete 25 out of 50 obvervations. Going to pick on the the
# higher petal widths. Creating rbinom vectors below with higher and lower
# probabilities of deletion

high_del <- rbinom(n = 25, size = 1, prob = 0.7)
low_del <- rbinom(n = 25, size = 1, prob = 0.3)

del_vec <- c(high_del, low_del)

# deleting based off of vectors above. Deleting based on petal.width (out
# outcome variable)

iris_available <- iris_2 %>% 
  arrange(desc(Petal.Width)) %>% 
  mutate(del_col = del_vec) %>% 
  mutate(Petal.Length = ifelse(del_col == 1, NA, Petal.Length))

```

## Part b

In the code below, I perform a regression of petal length on width, or x on y
for both the full data (mod_complete) frame and the available data frame
(mod_available). The models are relatively consistent. The constant for the full
model is 1.328 and the constant for the available model is 1.38. The coefficient
for petal.width for the full model is .546 and the coefficient for petal width
for the available data is .327. It makes sense that the two models are not
drastically different because the missing data is random with respext to petal
length.

```{r results="asis"}

# creating regression with full data but with y as predictor on x

mod_complete <- lm(Petal.Length ~ Petal.Width, data = iris_2)

mod_available <- lm(Petal.Length ~ Petal.Width, data = iris_available)

# the models are relatively similar because we didn't have any missing x values.
# our deletion was random with regard to petal length.

stargazer(mod_complete, mod_available, type = "latex")
```


## Part c

In the code below, I perform a regression of petal width on petal length, or y
on x, for both the full data frame (mod_complete_c) and the available data frame
(mod_available_c). The models are not consistent with each other. The constant
for the full model is -.048 and the constant for the available model is .084.
The coefficient for petal length for the full model is .201 and the coefficient
for petal length for the available model .097, less than half of the first
model. It makes sense that  these models are inconsistent because the missing
data is not random with respect to petal width.

```{r results="asis"}

mod_complete_c <- lm(Petal.Width ~ Petal.Length, data = iris_2)

mod_available_c <- lm(Petal.Width ~ Petal.Length, data = iris_available)

# the models are significantly different with regards to both the coefficients
# and the estimates.

stargazer(mod_complete_c, mod_available_c, type = "latex")

```

## Part d

The new model, shown in the third column of the stargazer table, has very different estimates from both the complete and partial models in part c. These differences show that how you treat missing data can have large effects for your model.

```{r results="asis"}

# using random imputation function from slides

random.imp.vec <- function(V)  {
  gone <- is.na(V)
  there <- V[!gone]
  V[gone] <- sample(x=there,size=sum(gone),replace=TRUE)
  return(V)
}

# creating the new data fram

df_d <- random.imp.vec(iris_available)

# creating the model with the imputed data

mod_d <- lm(Petal.Width ~ Petal.Length, data = df_d)

# comparing the models

stargazer(mod_complete_c, mod_available_c, mod_d,
          type = "latex")
```

# STAR data

```{r}
star98.missing <- read.table("star98.missing.dat.txt",header=TRUE) 
par(mfrow=c(1,2),mar=c(3,3,3,3)) 
plot(star98.missing$SUBSIDIZED.LUNCH,star98.missing$READING.ABOVE.50,pch="+",col="blue") 
abline(lm(star98.missing$READING.ABOVE.50~star98.missing$SUBSIDIZED.LUNCH),lwd=3) 
mtext(side=1,cex=1.3,line=2.5,"District Percent Receiving Subsidized Lunch") 
mtext(side=2,cex=1.3,line=2.5,"District Percent Above National Reading Median") 
plot(star98.missing$PTRATIO,star98.missing$READING.ABOVE.50,pch="+",col="blue") 
abline(lm(star98.missing$READING.ABOVE.50~star98.missing$PTRATIO),lwd=3) 
mtext(side=1,cex=1.3,line=2.5,"District Pupil/Teacher Ratio") 
mtext(side=2,cex=1.3,line=2.5,"District Percent Above National Reading Median") 
mtext(side=3,cex=1.5,outer=TRUE,line=-1,"Calfornia 9th Grade by District, 1998")

summary(star98.missing)

# how to check if there is a pattern? for there is subsidized lunch
```


## Part a

determine how much missing data there is and if there is a discernable pattern

Looking at the summary above there appears to be some missing data. 

* SUBSIDIZED.LUNCH has 90 NA's

* PTRATION has 104 NA's

* READING.ABOVE.50 has 106 NA's

To see if there is any discernable pattern I used the md.pattern() function from
the mice library (see output below). 

From it we can tell that there are 89
complete samples. 50 samples that just miss the READING.ABOVE.50, 49 samples
that just miss PTRATIO, 37 samples that just miss SUBSIDIZED.LUNCH, 25 samples
that only have SUBSIZED LUNCH, 23 samples that only have PTRATIO, 22 samples
that only have READING.ABOVE.50, and 8 samples that have no data.

```{r}

# shows up the pattern of missing data. From it we can tell that there are 89
# complete samples. 50 samples that just miss the READING.ABOVE.50, 49 samples
# that just miss PTRATIO, 37 samples that just miss SUBSIDIZED.LUNCH, 25 samples
# that only have SUBSIZED LUNCH, 23 samples that only have PTRATIO, 22 samples
# that only have READING.ABOVE.50, and 8 samples that have no data.

md.pattern(star98.missing)
```


## Parts b and c

In the code below, I first create a case wise deletion model and then a mice
model. The table below shows the results of the case wise deletion model on the
left and the mice model on the right. The estimates for the intercept differ by
about 5 points. The estimates for subsidized.lunch are almost equal, and the
esttimates for ptratio differ by about .2. Overall, the models are relatively
similar. The standard errors are a little bit smaller for the imputation model,
so I am inclined to say it is the better model.

```{r echo=FALSE}

# first have to create normal model using casewise deletion

mod_star <- lm(READING.ABOVE.50 ~ SUBSIDIZED.LUNCH + PTRATIO,
               na.action = na.omit,
               data = star98.missing)

# now using mice package to create multiple imputation model

# setting number to 5 like in lecture

m <- 5
imp.star <- mice(star98.missing, m)
star.mids <-  lm.mids(READING.ABOVE.50 ~ SUBSIDIZED.LUNCH + PTRATIO,
                      data = imp.star)


# creating table of model estimates next to each other where left is case wise
# deletion and right is mice

cbind(summary(mod_star)$coef[,1:2], summary(pool(star.mids))[,1:3]) %>%
  gt()
```

